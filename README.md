
# CIP Traffic Project

## Project Overview

This project demonstrates a **real-time data processing pipeline** for analyzing **US traffic congestion data** using modern big-data tools.  
It is designed to mimic how large-scale traffic systems handle **continuous streams of data**, rather than processing static files in batch mode.

The architecture reflects real-world streaming systems used in smart transportation and traffic monitoring applications.

---

## System Architecture and Workflow

### 1. Data Source (Traffic Dataset)

- The project uses a large historical dataset containing traffic congestion information from multiple regions across the United States.
- Instead of loading the entire dataset at once, the data is read in **small chunks**.
- This chunk-based reading simulates **real-time traffic updates** generated by sensors, cameras, or monitoring systems in real-world scenarios.

---

### 2. Kafka as the Messaging Layer

Apache Kafka acts as the **message broker** between the data producer and consumer.

- A **Kafka Producer** reads traffic data chunk-by-chunk from the dataset.
- Each chunk is converted into **JSON-formatted messages**.
- These messages are published to a Kafka topic.
- Kafka ensures:
  - Reliable message delivery  
  - Ordered data streams  
  - Fault tolerance and durability  

This design **decouples data generation from data processing**, enabling scalability and resilience in case of failures.

---

### 3. Dask for Distributed Processing

- A **Dask-based Consumer** subscribes to the Kafka topic.
- Incoming traffic messages are processed **in parallel** using Dask.
- Dask distributes the workload across multiple workers, enabling:
  - Faster processing of large volumes of traffic data  
  - Efficient use of system resources  

Typical operations include:
- Data filtering  
- Aggregations  
- Transformations  

Dask provides a scalable alternative to traditional single-machine processing and closely simulates **real big-data analytics environments**.

---

### 4. Near Real-Time Analytics Simulation

By continuously producing messages to Kafka and consuming them using Dask:

- The project replicates **near real-time data processing**.
- It demonstrates how modern **streaming data pipelines** are built and deployed.
- The architecture is representative of production systems used in:
  - Traffic monitoring  
  - Smart city platforms  
  - Transportation analytics  

---

### 5. Reproducible and Portable Design

The project is designed to be **portable, reproducible, and system-independent**:

- **Docker** is used to run Kafka, ensuring consistent setup across different machines.
- **Python virtual environments** manage dependencies cleanly.
- The dataset is downloaded dynamically using **KaggleHub**, instead of being stored in the repository.

This approach makes the project suitable for:
- Academic evaluation  
- Viva and demonstrations  
- Extension into real-world applications  

---
 

It includes:

- Kafka producer to send traffic data in chunks  
- Dask consumer for parallel data processing  
- Support for KaggleHub dataset download  

---

## Environment

- Python: 3.14.2  
- Messaging: Kafka  
- Processing: Dask  
- Dataset: US traffic congestion CSV (downloaded via KaggleHub)  

---

## Prerequisites

1. **Install Docker**  
   - Download and install Docker Desktop: https://www.docker.com/get-started  
   - Ensure Docker is running before starting Kafka or other services.

2. **Kafka setup using Docker**  
   - Create a `docker-compose.yml` file with Kafka and Zookeeper configuration (if not already provided).  
   - Start Kafka and Zookeeper:

```

docker-compose up -d

```



- Verify Kafka is running:

```

docker ps

```

---

## Setup

Follow these steps to set up the project environment:

### 1️⃣ Clone the project

```

git clone [https://github.com/Dhinesh-Manikandan/CIP-Project.git](https://github.com/Dhinesh-Manikandan/CIP-Project.git)
cd CIP-Project

```

### 2️⃣ Create Python virtual environment (Python 3.14.2)



#### Create virtual environment
```
python -m venv venv
```
#### Activate virtual environment (Windows PowerShell)
```
venv\Scripts\Activate.ps1
```
#### OR activate virtual environment (Windows Command Prompt)
```
venv\Scripts\activate.bat
```
#### OR activate virtual environment (Linux / MacOS)
```
source venv/bin/activate
```


### 3️⃣ Upgrade pip (optional but recommended)

```

python -m pip install --upgrade pip

```

### 4️⃣ Install required Python packages

```

pip install -r requirements.txt

```

> This installs all required dependencies, including:  
> `pandas`, `numpy`, `kafka-python`, `kagglehub`, `PyYAML`, `dask[complete]`, etc.

---

## Dataset

- Dataset is **not included** in the repository.  
- It is automatically downloaded using **KaggleHub** when you run the producer.  
- Ensure you have your Kaggle API credentials set up in your environment.

---

## Running the Project

### 1️⃣ Start Kafka Broker (Docker)

```

docker-compose up -d

```

- This starts Kafka and Zookeeper in detached mode (`-d`).  
- Ensure Kafka broker is accessible at `localhost:9092` or update the code accordingly.

### 2️⃣ Run the Producer

Open **terminal 1** and activate your virtual environment:

```

# Activate environment

venv\Scripts\Activate.ps1

```

Run the producer script:

```

python producer.py

```

- The producer reads CSV chunks and sends messages to the Kafka topic.  

### 3️⃣ Run the Dask Consumer

Open **terminal 2** and activate the **same virtual environment**:

```

# Activate environment

venv\Scripts\Activate.ps1

```

Run the consumer script:

```

python consumer_dask.py

```

- The Dask consumer processes messages from Kafka in parallel.  
- You can monitor progress and logs in the terminal.

---

## Logging

- Logs are automatically written to the `logs/` folder.  
- `producer.log` captures producer events.  
- `consumer.log` (if implemented) captures consumer events.  

> Note: Logs are **not tracked in Git** (see `.gitignore`).

---

## Notes

- Always run **producer first**, then the **consumer**.  
- Use the **same virtual environment** for all terminals.  
- If the project is run in a Docker container, ensure the container has Python 3.14.2 and the virtual environment is created inside it.  

---

## Clean Git Setup

- Virtual environment, data, and logs are excluded from Git.  
- All users can reproduce the project by cloning the repo and running the above setup steps.

---

## References

- [Kafka Python Documentation](https://kafka-python.readthedocs.io/en/master/)  
- [Dask Documentation](https://docs.dask.org/en/stable/)  
- [KaggleHub Documentation](https://pypi.org/project/kagglehub/)  
- [Docker Documentation](https://docs.docker.com/)
```

